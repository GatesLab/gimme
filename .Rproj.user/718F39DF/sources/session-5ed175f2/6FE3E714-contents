---
title: "Example Wright Analysis"
author: "kmg"
date: "2023-08-01"
output:   
  pdf_document: default
  html_document:
    df_print: paged
toc: yes
toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Set up the environment

We'll analyze a subset of the Wright data. We'll use the exploratory subgrouping option (S-GIMME) as well as include an exogeneous variable (linear time). 

```{r}
require(dplyr)
require(gimme)
require(perturbR)

WrightData_LF <-read.csv("/Users/kgates/Library/CloudStorage/GoogleDrive-katie.gates@gmail.com/My Drive/gimme workshop materials/Data/Wright/Daily Diary Long Form reduce.csv",  header = TRUE)

# WrightData_LF <-read.csv("/Users/gateskm/Library/CloudStorage/GoogleDrive-katie.gates@gmail.com/My Drive/gimme workshop materials/Data/Wright/Daily Diary Long Form reduce.csv", 
                         # header = TRUE)

WrightData<-split(WrightData_LF,WrightData_LF$ID)
```

# Prepare Data 

```{r}
# Select variables
# "SevereSUM" is the sum of how severe stressors were considered
# PA and NA. are positive affect and negative affect

WrightData_fewer <- WrightData
for (i in 1:length(WrightData_fewer))
  WrightData_fewer[[i]] <- WrightData_fewer[[i]] %>%
    select(Time, PA, NA., Urgency, SevereSUM, HostileDay)

### Check for low variance
N = 112 #Wright data
noVar = matrix(NA, N, 1)

for(i in 1:112)
  noVar[i] <- any(apply(WrightData_fewer[[i]], 2, stats::var, na.rm = TRUE) ==0)

# Wright data with fewer variables and variance check 
WrightData_fewerVar <- WrightData_fewer[which(noVar==FALSE)]

# Wright data with fewer variables and variance check and T>=90
WrightData_fewerVar90 <-WrightData_fewerVar[lapply(1:length(WrightData_fewerVar), function(x)
  length(complete.cases(WrightData_fewerVar[[x]])))>90]
```

# Run GIMME 

We'll run it with 'subgroup = TRUE' and the exogenous variable 'Time'. 
```{r, echo = TRUE, eval = FALSE}
outSubgroupWrightExog<- gimme(WrightData_fewerVar90,
                          subgroup = TRUE,
                          standardize = TRUE, 
                          out = "~/My Drive/gimme workshop materials/Exercises/WrightOut",
                          exogenous = "Time")

save(outSubgroupWrightExog, 
     file = "/Users/gateskm/Library/CloudStorage/GoogleDrive-katie.gates@gmail.com/My Drive/gimme workshop materials/Exercises/WrightOutput.Rdata")
plot(outSubgroupWrightExog)
```
```{r, echo = FALSE, eval = TRUE}

load("WrightOutput.Rdata")
plot(outSubgroupWrightExog)
```

We can see from the plot that there's a group level path from "SevereSUMM" (stress) to "NA.". 

We also see some subgroup-level paths. Let's explore further. 

# Explore GIMME output 

Let's take a look at the fit file. If we viewed the whole thing, we could see things like subgroup membership, fit, and convergence status for everyone. 

```{r}
head(outSubgroupWrightExog$fit)
```

## Look at subgroup results 

Now let's dive into the subgroups. 

```{r}
# see how many subgroups
length(unique(outSubgroupWrightExog$fit$sub_membership))
table(outSubgroupWrightExog$fit$sub_membership)

# Look at their paths 
plot(outSubgroupWrightExog$sub_plots_paths[[1]])
plot(outSubgroupWrightExog$sub_plots_paths[[2]])
```

## Explore Robustness of Subgroups

Let's see if there's evidence that these subgroups are reliably distinct from each other. 

### Explore VI
```{r}

# we use the similarity matrix from gimme: 

outPerturb <- perturbR(sym.matrix = outSubgroupWrightExog$sim_matrix, 
         plot       = TRUE, 
         errbars    = TRUE, 
         resolution = 0.01, # this can be increased to be faster
         reps       = 100)  # this can be decreased to be faster 

```

We can further probe to see if elements of the similarity matrix can be perturbed (rewired) at a relatively high proportion and still have community detection results be similar to the original membership assignments. 

```{r}
# obtain the VI value when 20% of community assignments randomly swapped:
outPerturb$vi20mark

# find when the alpha level (% perturbed) is greater than this value 

higherVI <- min(which(colMeans(outPerturb$VI)>outPerturb$vi20mark))
higherVI

# find alpha that corresponds with this 

outPerturb$percent[higherVI]

```

We see that here, the average VI is higher than our 20% mark at an alpha higher than .20. This looks very robust! 

Now lets check out modularity values. 

### Modularity investigations
```{r}
outPerturb$modularity[1,1] # value from original data

# We'll see if this is higher than expected by chance when compared with 
# a distribution of modularity estimates from similarity matrices having 
# the same properties as ours. 

hist(outPerturb$modularity[,which(round(outPerturb$percent, digits = 2) ==1.00)], xlim = c(0,1))
abline(v = outPerturb$modularity[1,1], col = "red")

```

This result suggests that our modularity isn't higher than chance. This contradicts our above results, so we can only tentatively say that our results are stable. 